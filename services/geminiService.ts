
import { GoogleGenAI, Type } from "@google/genai";
import { CodeReviewResult } from "../types";

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY || '' });

const REVIEW_SCHEMA = {
  type: Type.OBJECT,
  properties: {
    rating: {
      type: Type.STRING,
      description: "Code quality rating: Better, Good, Normal, or Bad",
    },
    summary: {
      type: Type.STRING,
      description: "Short overview of the code quality",
    },
    explanation: {
      type: Type.STRING,
      description: "Step-by-step explanation of what the code does",
    },
    issues: {
      type: Type.ARRAY,
      items: {
        type: Type.OBJECT,
        properties: {
          title: { type: Type.STRING },
          type: { type: Type.STRING, description: "bug | style | performance | security | readability | other" },
          description: { type: Type.STRING },
          fixExample: { type: Type.STRING }
        },
        required: ["title", "type", "description", "fixExample"]
      }
    },
    suggestions: {
      type: Type.ARRAY,
      items: { type: Type.STRING }
    },
    improvedCode: {
      type: Type.STRING,
      description: "The full improved version of the source code"
    },
    aiUsagePercentage: {
      type: Type.NUMBER,
      description: "An estimated percentage (0-100) of how likely this code was generated by an AI model"
    },
    originAnalysis: {
      type: Type.STRING,
      description: "A brief justification for the AI usage percentage"
    }
  },
  required: ["rating", "summary", "explanation", "issues", "suggestions", "improvedCode", "aiUsagePercentage", "originAnalysis"]
};

export const reviewCode = async (code: string, language: string): Promise<CodeReviewResult> => {
  const prompt = `You are an expert-level senior software engineer and AI code detection specialist.
I will give you some source code written in ${language}.

Your tasks:
1. Rate the overall code quality: "Better", "Good", "Normal", or "Bad".
2. Explain what the code does.
3. Identify bugs, security flaws, and performance bottlenecks.
4. Estimate the percentage (0-100) of AI authorship. Look for LLM-typical patterns (e.g., standard boilerplate, specific naming conventions, lack of typical human shortcuts/messiness).
5. Provide a justification for this percentage in 'originAnalysis'.
6. Provide an improved version.

Code to review:
\`\`\`${language.toLowerCase()}
${code}
\`\`\`
`;

  try {
    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: prompt,
      config: {
        responseMimeType: "application/json",
        responseSchema: REVIEW_SCHEMA,
      },
    });

    if (!response.text) {
      throw new Error("No response from AI");
    }

    return JSON.parse(response.text) as CodeReviewResult;
  } catch (error) {
    console.error("Gemini API Error:", error);
    throw error;
  }
};
